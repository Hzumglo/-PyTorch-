{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "114c203f-88a6-4ebd-b723-efbf58161627",
   "metadata": {},
   "source": [
    "# 1.数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a8db4b-791e-489a-9b74-4dae3bc9d91d",
   "metadata": {},
   "source": [
    "**张量**表示一个由数值组成的数组，这个数组可能由多个维度组成。具有一个维度的张量对应数学上的向量（一维数组），具有两个维度的张量对应数学上的矩阵（二维数组）。</br>\n",
    "我们可以使用**arange**创建一个行向量x。</br>\n",
    "## 1.1 基础操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28caaf4-8156-48fd-b98f-06506ddc67f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.arange(12)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2410dce0-e7cd-4435-9791-984030f5bf1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以通过张量的shape属性来访问张量的形状。\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90f9dc3-77fa-4301-9fd8-9b5d70cbc351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以通过size检查张量的大小\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb92277-9676-4ed9-931a-c83f35bfc085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以用reshape函数改变一个张量的形状而不改变元素数量和元素值\n",
    "X = x.reshape(3, 4)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5fb32a3-e6c3-4c07-86ea-4ae6af0386cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们，可以通过以下方式创建一个全0或全1的张量\n",
    "torch.zeros((2, 3, 4)) # 三维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4990d252-5322-4bb9-b9f1-783a83a5c886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]],\n",
       "\n",
       "        [[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones((2, 3, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf54f4d-b520-4a0b-861a-b9ba722d637a",
   "metadata": {},
   "source": [
    "有时我们想要通过从某个特定的概率分布中随机采样来得到张量中的每个元素的值。**例如，当我们构造数组来作为神经网络的参数时，我们通常会随机初始化参数的值，**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c2f8944-1e5f-472f-8d2c-c9e1dceeefce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0022, -1.1801,  0.4883,  0.3557],\n",
       "        [ 0.6891,  0.0223,  0.1559,  0.8406],\n",
       "        [-0.4567,  0.1438,  0.4234, -0.2717]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(3, 4)\n",
    "# 其中每个元素都从均值0、标准差为1的高斯分布中随机采样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64095275-c5f7-4d1f-87e2-c43b2bdff99b",
   "metadata": {},
   "source": [
    "## 1.2 运算符"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdda75a-bd25-4be0-9203-27d8d5a34501",
   "metadata": {},
   "source": [
    "对于任意具有相同形状的张量（**可以理解为矩阵的加减乘除**），常见的标准算术运算符（+、-、*、/和 \\*\\*）都可以被升级为按元素运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4955dd36-030b-43e0-8744-c63ea193b106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  4,  6, 10]) tensor([-2,  0,  2,  6]) tensor([ 0,  4,  8, 16]) tensor([0., 1., 2., 4.]) tensor([ 0,  4, 16, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0, 2, 4, 8])\n",
    "y = torch.tensor([2, 2, 2, 2])\n",
    "print(x+y, x-y, x*y, x/y, x**y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d720062-97f1-4ebb-b318-cf9b6b23c17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 求幂运算\n",
    "torch.exp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c1fb52-51dc-422e-8243-0bc4bd063657",
   "metadata": {},
   "source": [
    "我们还可以把多个张量连接在一起，只需要提供张量列表，并给出沿哪个轴连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac60461e-aab1-4b5c-bb70-f5d6a39b77bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.],\n",
       "         [ 2.,  1.,  4.,  3.],\n",
       "         [ 1.,  2.,  3.,  4.],\n",
       "         [ 4.,  3.,  2.,  1.]]),\n",
       " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "torch.cat((X, Y), dim = 0), torch.cat((X, Y), dim = 1)\n",
    "# dim = 0 沿x轴也就是行，反之亦然"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5be11dd9-bff7-4107-993f-33bde8855e41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False,  True, False,  True],\n",
       "         [False, False, False, False],\n",
       "         [False, False, False, False]]),\n",
       " tensor(66.))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还可以以逻辑运算符构建二元张量。\n",
    "X == Y, X.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1476dd1f-583c-4eae-addd-c223c7c99468",
   "metadata": {},
   "source": [
    "## 1.3 广播机制\n",
    "上述都是在两个张量形状相同的情况下进行的计算，而**广播机制**可以让我们在形状不同的前提下，也能使得两个张量进行运算。</br>\n",
    "这种机制的工作方式如下：</br>\n",
    "1.通过适当复制元素来扩展一个或两个数组，以便在转换后，两个张量具有相同形状；</br>\n",
    "2.对生成的数组执行按行元素操作。</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9a8b9b7-7e2a-4b0d-a4b8-50b67a702119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0],\n",
       "         [1],\n",
       "         [2]]),\n",
       " tensor([[0, 1]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(3).reshape((3, 1))\n",
    "b = torch.arange(2).reshape((1, 2))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dd27ebf-0220-4347-9b53-cde6062abcce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b # 让a、b都变成了3 * 2 的矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c6bf2-e12b-4b2e-b881-dd7275304ab8",
   "metadata": {},
   "source": [
    "## 1.4 索引和切片\n",
    "和python数组中一样，张量中的元素可以通过索引访问：第一个元素的所以是0，最后一个元素的索引是-1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00793afb-ec4c-4bf2-96e9-531883dbac3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([ 8.,  9., 10., 11.]),\n",
       " tensor([[ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, X[-1], X[1:3] # 左开右闭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10a8c6e9-d7c3-4c23-ba8f-0a7839c4d39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  9.,  7.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过指定索引将元素写入矩阵\n",
    "X[1, 2] = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa3a03eb-ec18-4a91-a3e1-f7389e872efe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[12., 12., 12., 12.],\n",
       "        [12., 12., 12., 12.],\n",
       "        [ 8.,  9., 10., 11.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还可以批量处理多个元素的值\n",
    "X[0:2, :] = 12 # 将第一行、第二行，所有列的元素全部改为12\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d462bd68-ba32-4329-8160-5f95ddbc1364",
   "metadata": {},
   "source": [
    "## 1.5节省内存\n",
    "执行一些操作可能会导致为结果新分配内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "856b52f5-c0d9-4e97-87ca-aac1ff9354de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(Y) # id()可以提供内存中引用对象的确切地址。\n",
    "Y = Y + X\n",
    "id(Y) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3a4ee-f352-45e4-bc95-d2112fdf96aa",
   "metadata": {},
   "source": [
    "这样的原因有两个：</br>\n",
    "1.我们不想总是不必要地分配内存。在机器学习中，我们可能有数百兆的参数，并且在一秒内多次更新所有的参数。通常情况下，我们希望原地执行这些更新；</br>\n",
    "2.**如果我们不原地更新，其他引用仍然会指向旧的内存位置，这样我们的某些代码可能会无意中引用旧的参数**。</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20ab0bb7-7f6d-4f3d-ab9e-89d3ce908bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id(Z): 2198600076880\n",
      "id(Z): 2198600076880\n"
     ]
    }
   ],
   "source": [
    "# 执行原地操作,我们可以使用切片表示法将操作的结果分配给先前的数组。\n",
    "Z = torch.zeros_like(Y)\n",
    "print('id(Z):', id(Z))\n",
    "Z[:] = X + Y\n",
    "print('id(Z):', id(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc6c2cd7-a5d9-44ed-aed2-606c4935dc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "before = id(X)\n",
    "X += Y # 与切片法的效果一致\n",
    "id(X) == before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589134d9-bc32-442a-acca-97a848e382b8",
   "metadata": {},
   "source": [
    "## 1.6 转换为其他Python对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec129d0c-858d-440e-99d8-a3d78fa49d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = X.numpy()\n",
    "B = torch.tensor(A)\n",
    "type(A), type(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "570ae1f3-c3de-4764-aacc-71434e3b0f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([3.5000]), 3.5, 3.5, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([3.5])\n",
    "a, a.item(), float(a), int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dd7416-a08a-4f8d-852a-3610944a9dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
